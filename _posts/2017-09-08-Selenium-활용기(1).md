---
layout: post
title:  "Selenium 활용기(1)"
date:   2017-09-80
excerpt: "Selenium 활용기(1)-크롤링(부제:정부기관 사이트는 정말...)"
project: true
tag:
- python
- 크롤링
- Selenium
- 프로젝트
- 데이터 크롤링
comments: true
---

<figure>
	<img src="https://huddle.eurostarsoftwaretesting.com/wp-content/uploads/2017/02/Selenium.jpg">
</figure>

* * *

그동안 Beautifulsoup를 이용한 크롤링을 자주 하다, 몇몇 멋진(?) 사이트들은 단순히 이전에 사용했던 라이브러리를 이용해 크롤링이 되지 않는 다는 것을 발견하였다.(우리나라 뿐 만 아니라, 몇몇 멋진 사이트들이 그러하다.... 자바스크립트로 데이터를 넣는 경우 또는 iframe 인 경우)


[e-금융민원센터](http://www.fcsc.kr/D/fu_d_08_04.jsp) 페이지를 열어서 Beautifulsoup를 사용하려 하였다.
페이지를 열어보면 역시나 iframe으로 데이터 테이블이 구성되어있다. 따라서 해당 테이블의 데이터를 뽑기 위해서는 직접 iframe 주소를 열어서 확인하는 수 밖에 없다.

우선 해당 iframe 주소를 Beautifulsoup를 이용해 열어보니, 구조는 그렇게 복잡하지 않았다. 하지만 Pagination으로 구성된 코드를 보면 다음과 같았다.

```
<div class="Lcenter">
	<a href="#first" onclick="alert('첫페이지 입니다.');"><img alt="처음페이지로 이동" src="/kr/integration/img/theme01/paging_first.gif"/></a>
	<a href="#prev" onclick="alert('이전페이지 세트가 없습니다.');"><img alt="이전 페이지세트로 이동" src="/kr/integration/img/theme01/paging_before.gif"/></a>
	<a href="#paging" onclick="paging('1'); return false;"><span class="first Lcurrent">1</span></a>
	<a href="#paging" onclick="paging('2'); return false;"><span>2</span></a>
	<a href="#paging" onclick="paging('3'); return false;"><span>3</span></a>
	<a href="#paging" onclick="paging('4'); return false;"><span>4</span></a>
	<a href="#paging" onclick="paging('5'); return false;"><span>5</span></a>
	<a href="#paging" onclick="paging('6'); return false;"><span>6</span></a>
	<a href="#paging" onclick="paging('7'); return false;"><span>7</span></a>
	<a href="#paging" onclick="paging('8'); return false;"><span>8</span></a>
	<a href="#paging" onclick="paging('9'); return false;"><span>9</span></a>
	<a href="#paging" onclick="paging('10'); return false;"><span>10</span></a>
	<a class="next" href="#next" onclick="paging('11'); return false;"><img alt="다음 페이지세트로 이동" src="/kr/integration/img/theme01/paging_next.gif"/></a>
	<a href="#last" onclick="paging('152'); return false;"><img alt="마지막 페이지로 이동" src="/kr/integration/img/theme01/paging_last.gif"/></a>
</div>
```
Pagination 코드 부분이 onclick 이벤트로 함수를 불러서 이동하는 것이다. 그리고 10개 단위로 '다음 페이지 세트'로 이동도 해줘야한다. 이럴 경우 단순한 HTML 코드로 전체 데이터를 긁어오는 것이 무리다. 따라서 Beautifulsoup가 아닌 Selenium을 이용해 데이터를 긁어오도록 하였다.


#### Selenium 코드 (Python2.7 기준)

```
# -*- coding: utf-8 -*-
import urllib2
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import csv

hdr = {'User-Agent': 'Mozilla/5.0', 'referer': 'http://www.google.com'}
path = "/PATH-TO-/chromedriver"
urls = "http://61.73.100.30/kr/bbs/iframe/list.jsp?bbsid=1273209703694"

# csv open
f = open('output.csv', 'w')
wr = csv.writer(f)
wr.writerow([u'#',u'상호'.encode('utf-8'),u'홈페이지'.encode('utf-8'),u'대표자'.encode('utf-8'),u'모바일'.encode('utf-8')])

driver = webdriver.Chrome(path)
driver.get(urls)

a_link = 3
for each in range(1,153):
    if each != 152:
        sections = range(1,11)
    else:
        sections = range(1,5)
    for li in sections:
        ids = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[1]').text
        title = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[2]').text
        webs = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[3]').text
        name = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[4]').text
        mobile = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[5]').text
        wr.writerow([ids.encode('utf-8'),title.encode('utf-8'),webs.encode('utf-8'),name.encode('utf-8'),mobile.encode('utf-8')])

    if a_link == 13:
        a_link = 4

    else:
        a_link += 1        
    
    on_html_bt = driver.find_element_by_xpath('//*[@id="content"]/div[3]/div/a[%s]' %(a_link))
    on_html_bt.click()
    
print "-----fin-----"

f.close()

```

우선 전체적인 코드는 위와 같다. 하나씩 뜯어서 분석해보자.

### 1) import 라이브러리
```
# -*- coding: utf-8 -*-
import urllib2
from selenium import webdriver
from selenium.webdriver.support.ui import Select
import csv
```

기본적으로 selenium을 통해 자동으로 웹 사이트에 들어가는 것이므로 urllib2 모듈과 selenium을 불러온다.
편하게 설치하기 위해서는 pip 로 설치하는게 좋다.

> pip install selenium
[설치 DOC](http://selenium-python.readthedocs.io/installation.html)
로 설치하자. 그리고 추가적으로 설치해야하는 드라이버가 있는데, 개인의 취향(?)에 따른 드라이버를 다운받는다.
webdriver를 통해 우리가 사용하는 노트북,컴퓨터의 브라우저를 자동 제어할 수 있기 위함이다.

<figure>
	<img src="https://github.com/CodeMath/codemath.github.io/blob/master/assets/img/media/selenium_0.png?raw=true">
</figure>
webdriver 폴더에 들어가면 다양한 브라우저를 지원하는 것을 알 수 있다.

보통 윈도우/맥 유저 편하게 사용할 수 있는 크롬 브라우저 웹 드라이버를 다운받자.(아니면 파이어폭스)
[Chrome driver 다운로드 링크](https://sites.google.com/a/chromium.org/chromedriver/downloads)
[FireFox driver 다운로드 링크](https://github.com/mozilla/geckodriver)



### 2) 접속할 URL setting
```
hdr = {'User-Agent': 'Mozilla/5.0', 'referer': 'http://www.google.com'}
path = "/PATH-TO-/chromedriver"
urls = "http://61.73.100.30/kr/bbs/iframe/list.jsp?bbsid=1273209703694"
```
<figure>
	<img src="https://github.com/CodeMath/codemath.github.io/blob/master/assets/img/media/crawling_website.png?raw=true">
</figure>

헤더 정보에 'User-Agent' 부분에 'Mozilla/5.0'로 입력하고 'referer'에는 그냥 'http://www.google.com' 입력하자.

사실 'User-Agent'는 쉽게 말해 브라우저의 종류를 판단하는 것으로 사용되어왔다.(물론 워낙 브라우저 종류가 많아서, 판단하기가 쉽지는 않지만.) 대표적인 브라우저 몇몇의 특징이 있다. 그동안 브라우저나 OS 판단 용으로 이해하고 있었는데 이번 기회에 확실하게 공부하였다.

자세한 내용은 다음 사이트를 참고하면 된다.(물론 크롤링하기 위해서 다 알아야할 필요는 없지만...) [User-Agent 설명 잘 된 사이트 링크](http://ohgyun.com/292)

또한 'referer'는 로그분석이나 접근제어할 때 주로 사용한다. 쉽게 말해 어디에서 이 사이트로 들어왔나? 라는 정보를 가지고 온다고 생각하면 된다.

그리고 urls 변수에 크롤링할 URL을 입력한다.




### 3) 접속할 URL setting
```
# csv open
f = open('output.csv', 'w')
wr = csv.writer(f)
wr.writerow([u'#',u'상호'.encode('utf-8'),u'홈페이지'.encode('utf-8'),u'대표자'.encode('utf-8'),u'모바일'.encode('utf-8')])

driver = webdriver.Chrome(path)
driver.get(urls)
```

우리가 뽑을 데이터를 단순히 print 하기에는 아까우니(?) CSV 형태로 뽑아서 정리하자. python에는 기본적으로 csv로 뽑을 수 있는 모듈이 있으므로 별도 설치할 필요없이 바로 불러온다.


새로운 csv 파일을 만들기 위해서는 주석을 제외한 첫 번째 코드를 보면 된다.
> f = open('output.csv', 'w')
여기서 open함수에는 읽기/쓰기/수정 등등의 권한을 명시하여 사용하면 된다. 우리는 쓰기 권한이 필요하므로 'w' = 'write'. 

csv부분에 대한 자세한 공부는 [다음 링크](https://docs.python.org/2/library/csv.html)를 참조하자.


> wr = csv.writer(f)
이 코드는 앞서 열었던 csv 파일(f)에 작성을 한다는 뜻이다.

> wr.writerow([u'#',u'상호'.encode('utf-8'),u'홈페이지'.encode('utf-8'),u'대표자'.encode('utf-8'),u'모바일'.encode('utf-8')])
그리고 첫 번째 row에는 table에서 뽑을 데이터 구분을 위해 미리 1줄을 입력하자. 'writerow'를 통해 한 줄을 작성한다. 

<figure>
	<img src="https://designedbynatalie.files.wordpress.com/2011/05/asciierrorsgraphic.gif?raw=true">
</figure>

그런데 python2.7 버전에서는 한글을 쓰면 아스키 에러를 토해낸다.(그러게 python 3으로 넘어갔어야지...ㅠㅠ)
급하게 코드를 만들어야 했기 때문에 어쩔 수 없이 각 한글에 .encode('utf-8') 을 붙여주자.


```
driver = webdriver.Chrome(path)
driver.get(urls)
```
그리고 앞서 설정한 크롬 브라우저 드라이버의 경로값을 넣어서 webdriver를 불러오자. 크롤링할 웹 사이트로 이동하자.(get method)




### 4) Javascript 잡아내기
```
a_link = 3
for each in range(1,153):
    if each != 152:
        sections = range(1,11)
    else:
        sections = range(1,5)
    for li in sections:
        ids = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[1]').text
        title = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[2]').text
        webs = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[3]').text
        name = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[4]').text
        mobile = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[5]').text
        wr.writerow([ids.encode('utf-8'),title.encode('utf-8'),webs.encode('utf-8'),name.encode('utf-8'),mobile.encode('utf-8')])

    if a_link == 13:
        a_link = 4

    else:
        a_link += 1        
    
    on_html_bt = driver.find_element_by_xpath('//*[@id="content"]/div[3]/div/a[%s]' %(a_link))
    on_html_bt.click()
    
print "-----fin-----"

f.close()
```
우선 전체 크롤링할 페이지가 152페이지인 것을 알 고 있으니, range(1,153)으로 설정하자.


```
    
    if each != 152:
        sections = range(1,11)
    else:
        sections = range(1,5)
    for li in sections:
    	...
```
그리고 마지막 페이지에서 크롤링할 데이터가 4개 이므로 152일 때 range(1,5)로 설정했다.


```
for li in sections:
	ids = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[1]').text
	title = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[2]').text
	webs = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[3]').text
	name = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[4]').text
	mobile = driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[5]').text
	wr.writerow([ids.encode('utf-8'),title.encode('utf-8'),webs.encode('utf-8'),name.encode('utf-8'),mobile.encode('utf-8')])
```
이제 sections 기준으로 for loop를 돌리면 된다. 여기서 selenium flow를 설명을 하면 다음과 같다.


```
driver = webdriver.Chrome(path) # 크롬 드라이버 실행(자동 제어)
driver.get(urls) # url 이동(get)
driver.find_element_by_xpath('//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[1]').text # 이동 페이지에서 element를 찾는다.
```

이동할 페이지에서 element를 찾을 때 id 값이나 class나 다양한 방법으로 처리할 수 있지만, 가장 편한 방법은 xpath로 찾는 것이다. (편함 x 1000000)

<figure>
	<img src="https://designedbynatalie.files.wordpress.com/2011/05/selenium_1.png?raw=true">
</figure>

크롬 브라우저 기준으로 크롤링할 데이터가 있는 부분에 마우스 우클릭->검사->Copy->Copy XPath 로 가져오면 된다.
> '//*[@id="content"]/div[2]/table/tbody/tr['+str(li)+']/td[1]'
앞서 가져온 XPath를 분석해보면 id값 기준으로 트리를 타는 듯 한 느낌이 보인다. ('/' 기준으로 트리로 자식안으로 쉽게 들어가니까...)

여튼 다시 본론으로 들어가면, 각 페이지마다 10개의 데이터가 있다. 따라서 loop를 돌릴 때, 각각의 데이터들에 맞게 위치를 맞춰서 돌리면 된다.

> wr.writerow([ids.encode('utf-8'),title.encode('utf-8'),webs.encode('utf-8'),name.encode('utf-8'),mobile.encode('utf-8')])

그리고 뽑은 데이터를 앞서 만든 csv에 1개 row로 넣는다.


```
if a_link == 13:
	a_link = 4

else:
	a_link += 1        
    
on_html_bt = driver.find_element_by_xpath('//*[@id="content"]/div[3]/div/a[%s]' %(a_link))
on_html_bt.click()
```
앞서 Pagination 부분을 보면 onclick event로 넘어가는 것을 확인했다. 따라서 다음 페이지로 이동할 때를 생각하면 다음과 같은 로직으로 정리된다. 처음에는 1페이지이지만, XPath 는 
> '//*[@id="content"]/div[3]/div/a[3]'
즉, 3번째 a 테그 부터가 1,11,21,31... 로 시작하는 것이다. 따라서 맨 처음 시작을 a_link라는 변수에 3으로 담아서 시작한다.

첫 번째 페이지에서 데이터를 뽑으면 a_link는 3이지 13이 아니므로, +=1이 되어 4가 되고 2번째 페이지로 이동한다.
이 과정이 계속 일어나면 10번째 페이지로 이동할 때는 a_link는 12이 된다. 10번 째 페이지의 데이터를 뽑고 나서 a_link는 12이므로 역시 +=1이되어 13이 되고 13번째 a 테그가 있는 부분인 '다음 페이지 세트'로 넘어가게 된다.
(이제 페이지 세트가 바뀌게되었다. 1,2,3,4.. -> 11,12,13,14...)페이지를 이동하면 바로 11페이지로 이동하므로 해당 데이터를 뽑는다. 

그리고 앞서 a_link는 13이라고 했으므로 11페이지에서 12페이지로 이동해야한다. 따라서 a_link를 4로 다시 할당한다.
이러한 과정이 1페이지 부터 152페이지까지 계속 도는 것이다.


```
print "-----fin-----"

f.close()
```
마지막으로 앞서 열었던 csv파일인 f를 닫아준다. (print 는 끝났다는 신호로 사용)


최종 정리된 데이터가 들어있는 csv파일을 열어보면 이쁘게 정리되어있는 것을 확인 할 수 있다.



해당 프로젝트 다운로드 링크 : [Jupyter notebook]("https://github.com/CodeMath/codemath.github.io/blob/master/assets/img/media/selenium_crawling_table.ipynb")


